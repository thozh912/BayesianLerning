---
title: "Lab 3 report"
author: "Andrea Bruzzone, Thomas Zhang"
output: pdf_document
---

##Assignment 1
#1.a
Assuming that the daily precipitation are independent normally distributed with parameters $\mu$ and $\sigma^2$ we use a Gibbs sampler to simulate from the joint posterior $p(\mu, \sigma^2|y_1,..,y_n)$

Let see if the sampler converges, starting with the traceplot of $\mu$:

```{r, echo=FALSE}
raindata <- read.table(file = "https://raw.githubusercontent.com/STIMALiU/BayesLearnCourse/master/Labs/rainfall.dat")
#head(raindata)
n <- dim(raindata)[1]

#This is a draw from Inv-chisq(n-1,ssquared)
invchisqaccordingtoslide <- function(n,ssquared){
  eks <- rchisq(1, df = n - 1)
  sigmasquared <- (n - 1) * ssquared / eks
  return(sigmasquared)
}


sigmasquared <- rep(0,1000)
mu <- rep(30,1000)
mu[1] <- 30
sigmasquared[1] <- 1000

ybar <- mean(raindata[,1])
ssquared <- var(raindata[,1])


tausquared_0 <- 300

mu_0 <- 20

nu_0 <- 4
sigmasquared_0 <- 40

j <- 2
while(j <  1001){
    w <- (n / sigmasquared[j-1]) / ((n / sigmasquared[j-1]) + 1 / tausquared_0) 
    tausquared_n <- solve(n / sigmasquared[j-1] + 1 / tausquared_0)
    mu_n <- w * ybar + ( 1 - w) * mu_0
    mu[j] <- rnorm(1, mean = mu_n, sd = sqrt( tausquared_n))
    secondtermbelow <- (nu_0 * sigmasquared_0 + sum((raindata - mu[j])^2)) / (n + nu_0)
    sigmasquared[j] <- invchisqaccordingtoslide(n,secondtermbelow)
    j <- j + 1
}

plot(mu, type = "l", main = "Traceplot of mu")
```

From the traceplot $\mu$ seems to converge to a value between 32 and 33. For further analysis we compute the ACF and the inefficiency factor:

```{r, echo=FALSE}
muac <- acf(mu, main = "ACF of traceplot of mu")
(sum(muac$acf[2:31])*2)+1 #the IF
```

They both shows that the sampler is very efficience since the draws seem to be independent.

The traceplot of $\sigma^2$:

```{r, echo=FALSE}
plot(sigmasquared, type = "l", main = "Traceplot of sigma squared")
```

Also $\sigma^2$ seems to converge, to a value around 1550. The ACF and inefficiency factor:

```{r, echo=FALSE}
sigcf <- acf(sigmasquared, main = "ACF of traceplot of sigma squared")
(sum(sigcf$acf[2:31])*2)+1 #the IF
```

As for $\mu$, we can say that the draws seem to be independent. In general the Gibbs sampler is very good.

In this final plot we report the histogram of the data together with the normal distribution using the posterior mean values for the two parameters:

```{r, echo=FALSE}
x <- as.matrix(raindata['V1'])
xGrid <- seq(min(x)-1*apply(x,2,sd),max(x)+1*apply(x,2,sd),length = 100)
xGridMin <- min(xGrid)
xGridMax <- max(xGrid)
ndistr <- dnorm(xGrid , mean = mu[1000], sd = sqrt(sigmasquared[1000]))

hi <- hist(x, breaks = 20, freq = FALSE, xlim = c(xGridMin,xGridMax), main = "Histogram of Japan daily precipitation", ylab = "")
lines(xGrid, ndistr, col = "red")
legend("topright", box.lty = 1, legend = c("Data histogram",'Normal Model'), 
                                          col = c("black", 'red'), lwd = 2)

```

#1.b

Using mixture normal model, here we present the plot for the last iteration with the two components. 

```{r, echo=FALSE}
x <- as.matrix(raindata['V1'])

j <- NULL
# Model options
nComp <- 2    # Number of mixture components

# Prior options
alpha <- 10*rep(1,nComp) # Dirichlet(alpha)
muPrior <- rep(30,nComp) # Prior mean of theta
tau2Prior <- rep(300,nComp) # Prior std theta
sigma2_0 <- rep(var(x),nComp) # s20 (best guess of sigma2)
nu0 <- rep(4,nComp) # degrees of freedom for prior on sigma2

# MCMC options
nIter <- 10000 # Number of Gibbs sampling draws

# Plotting options
plotFit <- FALSE
lineColors <- c("blue", "green", "magenta", 'yellow')
sleepTime <- 0.000001 # Adding sleep time between iterations for plotting
################   END USER INPUT ###############

###### Defining a function that simulates from the 
rScaledInvChi2 <- function(n, df, scale){
  return((df*scale)/rchisq(n,df=df))
}

####### Defining a function that simulates from a Dirichlet distribution
rDirichlet <- function(param){
  nCat <- length(param)
  thetaDraws <- matrix(NA,nCat,1)
  for (j in 1:nCat){
    thetaDraws[j] <- rgamma(1,param[j],1)
  }
  thetaDraws = thetaDraws/sum(thetaDraws) # Diving every column of ThetaDraws by the sum of the elements in that column.
  return(thetaDraws)
}

# Simple function that converts between two different representations of the mixture allocation
S2alloc <- function(S){
  n <- dim(S)[1]
  alloc <- rep(0,n)
  for (i in 1:n){
    alloc[i] <- which(S[i,] == 1)
  }
  return(alloc)
}

# Initial value for the MCMC
nObs <- length(x)
S <- t(rmultinom(nObs, size = 1 , prob = rep(1/nComp,nComp))) # nObs-by-nComp matrix with component allocations.
theta <- vector("list", nIter)
theta[[1]] <- quantile(x, probs = seq(0,1,length = nComp))
sigma2 <- vector("list", nIter)
sigma2[[1]] <- rep(var(x),nComp)
probObsInComp <- rep(NA, nComp)
pi <- vector("list", nIter)

# Setting up the plot
xGrid <- seq(min(x)-1*apply(x,2,sd),max(x)+1*apply(x,2,sd),length = 100)
xGridMin <- min(xGrid)
xGridMax <- max(xGrid)
mixDensMean <- rep(0,length(xGrid))
effIterCount <- 0
ylim <- c(0,2*max(hi$density))


for (k in 1:nIter){
  #message(paste('Iteration number:',k))
  alloc <- S2alloc(S) # Just a function that converts between different representations of the group allocations
  nAlloc <- colSums(S)
  #print(nAlloc)
  # Update components probabilities
  w <- rDirichlet(alpha + nAlloc)
  for(j in 1:nComp){
   pi[[k]][[j]] <- w[j,]  
  }
  
  # Update theta's
  for (j in 1:nComp){
    precPrior <- 1/tau2Prior[j]
    precData <- nAlloc[j]/sigma2[[k]][[j]]
    precPost <- precPrior + precData
    wPrior <- precPrior/precPost
    muPost <- wPrior*muPrior + (1-wPrior)*mean(x[alloc == j])
    tau2Post <- 1/precPost
    theta[[k+1]][[j]] <- rnorm(1, mean = muPost, sd = sqrt(tau2Post))
  }
  
  # Update sigma2's
  for (j in 1:nComp){
    sigma2[[k+1]][[j]] <- rScaledInvChi2(1, df = nu0[j] + nAlloc[j], scale = (nu0[j]*sigma2_0[j] + sum((x[alloc == j] - theta[[k+1]][[j]])^2))/(nu0[j] + nAlloc[j]))
  }
  
  # Update allocation
  for (i in 1:nObs){
    for (j in 1:nComp){
      probObsInComp[j] <- w[j]*dnorm(x[i], mean = theta[[k]][[j]], sd = sqrt(sigma2[[k]][[j]]))
    }
    S[i,] <- t(rmultinom(1, size = 1 , prob = probObsInComp/sum(probObsInComp)))
  }
  
  # Printing the fitted density against data histogram
  if (plotFit && k == nIter){
    effIterCount <- effIterCount + 1
    hist(x, breaks = 20, freq = FALSE, xlim = c(xGridMin,xGridMax), main = paste("Iteration number",k), ylim = ylim)
    mixDens <- rep(0,length(xGrid))
    components <- c()
    for (j in 1:nComp){
      compDens <- dnorm(xGrid,theta[[k]][[j]],sd = sqrt(sigma2[[k]][[j]]))
      mixDens <- mixDens + w[j]*compDens
      lines(xGrid, compDens, type = "l", lwd = 2, col = lineColors[j])
      components[j] <- paste("Component ",j)
    }
    mixDensMean <- ((effIterCount-1)*mixDensMean + mixDens)/effIterCount
    
    lines(xGrid, mixDens, type = "l", lty = 2, lwd = 3, col = 'red')
    legend("topright", box.lty = 1, legend = c("Data histogram",components, 'Mixture'), 
           col = c("black",lineColors[1:nComp], 'red'), lwd = 2)
    Sys.sleep(sleepTime)
  }
  
}

  
```

Let check the convergence and the efficiency of all the parameters, we decide to choose 100 as burnin.

Traceplot, ACF and inefficiency factor of $\mu_1$:

```{r, echo=FALSE}

theta1 <- c()
for(i in 1:nIter){
 theta1[i] <- theta[[i]][[1]]
}
plot(theta1[100:nIter], type ="l" ,main = "Traceplot of theta1")
t1ac <- acf(theta1[100:nIter], main = "ACF")
paste("IF: ",round((sum(t1ac$acf[2:39])*2)+1,2)) #the IF
```

$\mu_1$ seems to converge to a value around 11 but the draws seem to be highly positively correlated, which leads to a high IF, which means that the number of equivalent independent draws is merely a fraction of the actual draws.

Traceplot, ACF and inefficiency factor of $\mu_2$:

```{r, echo=FALSE}

theta2 <- c()
for(i in 1:nIter){
  theta2[i] <- theta[[i]][[2]]
}
plot(theta2[100:nIter], type = "l", main = "Traceplot of theta2")
ac <- acf(theta2[100:nIter], main = "ACF")
paste("IF: ",round((sum(ac$acf[2:39])*2)+1, 2)) #the IF
```

$\mu_2$ seems to converge to a value around 61 but the draws seem to be positively correlated.
The IF number is rather large here as well.

Traceplot, ACF and inefficiency factor of $\sigma^2_1$:

```{r, echo=FALSE}
sigma1 <- c()
for(i in 1:nIter){
  sigma1[i] <- sigma2[[i]][[1]]
}
plot(sigma1[100:nIter], type ="l", main = "Traceplot of sigma1")
sig1 <- acf(sigma1[100:nIter], main = "ACF")
paste("IF: ",round((sum(sig1$acf[2:39])*2)+1,2)) #the IF
```

$\sigma^2_1$ converges to a value close to 86, the draws seem to be highly positively correlated, which leads to a high IF, which means that the number of equivalent independent draws is merely a fraction of the actual draws.

Traceplot, ACF and inefficiency factor of $\sigma^2_2$:

```{r, echo=FALSE}
sigmaSecond <- c()
for(i in 1:nIter){
  sigmaSecond[i] <- sigma2[[i]][[2]]
}
plot(sigmaSecond[100:nIter], type = "l", main = "Traceplot of sigma2")
sig2 <- acf(sigmaSecond[100:nIter], main = "ACF")
paste("IF: ",round((sum(sig2$acf[2:39])*2)+1,2)) #the IF
```

$\sigma^2_2$ converges to a value close to 2100. The draws are not really correlated and the IF value is quite small.

Traceplot, ACF and inefficiency factor of $\pi_1$:

```{r, echo=FALSE}
pi1 <- c()
for(i in 1:nIter){
  pi1[i] <- pi[[i]][[1]]
}
plot(pi1[100:nIter], type ="l", main = "Traceplot of pi1")
p1 <- acf(pi1[100:nIter], main = "ACF")
paste("IF: ",round((sum(p1$acf[2:39])*2)+1,2)) #the IF
```

$\pi_1$ converges to a value that is around 0.57 and the draws seems to be highly positively correlated, with a high IF value.

Traceplot, ACF and inefficiency factor of $\pi_2$:

```{r, echo=FALSE}
pi2 <- c()
for(i in 1:nIter){
  pi2[i] <- pi[[i]][[2]]
}
plot(pi2[100:nIter], type="l", main = "Traceplot of pi2")
p2 <- acf(pi2[100:nIter], main = "ACF")
paste("IF: ",round((sum(p2$acf[2:39])*2)+1,2)) #the IF
```

Also $\pi_2$ converges to a value around 0.43, but in this case from the ACF and IF we can see that the draws seem to be highly positively correlated.

#1.c
Histogram of the data with the model in 1.a and 1.b using the posterior mean values for all the parameters:

```{r, echo=FALSE}
hist(x, breaks = 20, freq = FALSE, xlim = c(xGridMin,xGridMax), main = "Final fitted density")
lines(xGrid, mixDens, type = "l", lwd = 2, lty = 4, col = "red")
lines(xGrid, ndistr, type = "l", lwd = 2, col = "blue")
legend("topright", box.lty = 1, legend = c("Data histogram","Mixture density","Normal density"), col=c("black","red","blue"), lwd = 2)
#the mixture seems to be the best model in terms of fitting
```

The mixture of normal models seems to be the best in terms of fitting.

##Assignment 2


We are going to use a gibbs sampler to find the regression coefficients to be used in the probit regression model and then compare the performance of the regression coefficients $\beta$ to regression coefficients drawn from $\mathcal{N}(\tilde{\beta},J^{-1})$.

```{r, echo=FALSE, warning=FALSE}
###########   BEGIN USER INPUTS   ################
Probit <- 1 # If Probit <-0, then logistic model is used.
chooseCov <- c(1:16) # Here we choose which covariates to include in the model
tau <- 10; # Prior scaling factor such that Prior Covariance = (tau^2)*I
###########     END USER INPUT    ################


# install.packages("mvtnorm") # Loading a package that contains the multivariate normal pdf
library("mvtnorm") # This command reads the mvtnorm package into R's memory. NOW we can use dmvnorm function.
library("msm")
# Loading data from file
Data<-read.table("SpamReduced.dat",header=TRUE)  # Spam data from Hastie et al.
y <- as.vector(Data[,1]); # Data from the read.table function is a data frame. Let's convert y and X to vector and matrix.
X <- as.matrix(Data[,2:17]);
covNames <- names(Data)[2:length(names(Data))];
X <- X[,chooseCov]; # Here we pick out the chosen covariates.
covNames <- covNames[chooseCov];
nPara <- dim(X)[2];

# Setting up the prior
mu <- as.vector(rep(0,nPara)) # Prior mean vector
Sigma <- tau^2*diag(nPara);

# Defining the functions that returns the log posterior (Logistic and Probit models). Note that the first input argument of

# this function must be the one that we optimize on, i.e. the regression coefficients.

LogPostLogistic <- function(betaVect,y,X,mu,Sigma){
  nPara <- length(betaVect);
  linPred <- X%*%betaVect;
                                      
  logLik <- sum( linPred*y -log(1 + exp(linPred)));
  if (abs(logLik) == Inf) logLik = -20000; # Likelihood is not finite, stear the optimizer away from here!
  logPrior <- dmvnorm(betaVect, matrix(0,nPara,1), Sigma, log=TRUE);
  return(logLik + logPrior)
}

LogPostProbit <- function(betaVect,y,X,mu,Sigma){
  nPara <- length(betaVect);
  linPred <- X%*%betaVect;
                                      
  # MQ change:
  # instead of logLik <- sum(y*log(pnorm(linPred)) + (1-y)*log(1-pnorm(linPred)) ) type in the equivalent and
  # much more numerically stable; 
  logLik <- sum(y*pnorm(linPred, log.p = TRUE) + (1-y)*pnorm(linPred, log.p = TRUE, lower.tail = FALSE))
  # The old expression: logLik2 <- sum(y*log(pnorm(linPred)) + (1-y)*log(1-pnorm(linPred)) )
  abs(logLik) == Inf
  #print('-----------------')
  #print(logLik)
  #print(logLik2)
  #if (abs(logLik) == Inf) logLik = -20000; # Likelihood is not finite, stear the optimizer away from here!
  logPrior <- dmvnorm(betaVect, matrix(0,nPara,1), Sigma, log=TRUE);
  return(logLik + logPrior)
}

# Calling the optimization routine Optim. Note the auxilliary arguments that are passed to the function logPost
# Note how I pass all other arguments of the function logPost (i.e. all arguments except betaVect which is the one that we are trying to optimize over) to the R optimizer.
# The argument control is a list of options to the optimizer. Here I am telling the optimizer to multiply the objective function (i.e. logPost) by -1. This is because
# Optim finds a minimum, and I want to find a maximum. By reversing the sign of logPost I can use Optim for my maximization problem.

# Different starting values. Ideally, any random starting value gives you the same optimum (i.e. optimum is unique)
initVal <- as.vector(rep(0,dim(X)[2])); 
# Or a random starting vector: as.vector(rnorm(dim(X)[2]))
# Set as OLS estimate: as.vector(solve(crossprod(X,X))%*%t(X)%*%y); # Initial values by OLS

if (Probit==1){
  logPost = LogPostProbit;
} else{
  logPost = LogPostLogistic;
}
  
OptimResults<-optim(initVal,logPost,gr=NULL,y,X,mu,Sigma,method=c("BFGS"),control=list(fnscale=-1),hessian=TRUE)
# 
# # Printing the results to the screen
# names(OptimResults$par) <- covNames # Naming the coefficient by covariates
# approxPostStd <- sqrt(diag(-solve(OptimResults$hessian))) # Computing approximate standard deviations.
# names(approxPostStd) <- covNames # Naming the coefficient by covariates
# print('The posterior mode is:')
# print(OptimResults$par)
# print('The approximate posterior standard deviation is:')
# approxPostStd <- sqrt(diag(-solve(OptimResults$hessian)))
# print(approxPostStd)
drawnbetaposterior <- rmvnorm(1, mean = OptimResults$par, sigma = solve(-OptimResults$hessian))

m <- 10000
betanow <- rmvnorm(1, sigma = 100 * diag(16))
betanow <- as.vector(betanow)
betanows <- matrix(0,nrow = m, ncol = 16)
betanows[1,] <- betanow
j <- 2

while(j <  m + 1){
  ueyes <- matrix(0, nrow = 1,ncol = length(y))
  for( i in 1:length(y)){
    if(y[i] == 0){
      ueyes[i] <- rtnorm(1, mean = X[i,] %*% betanows[j-1,], upper = 0)    
    } else{
      ueyes[i] <- rtnorm(1, mean = X[i,] %*% betanows[j-1,], lower = 0)
    }
  }
  ueyes <- as.vector(ueyes)
  #themvmodel <- as.data.frame(cbind(resp = ueyes, X))
  betanow <- lm(ueyes ~ X - 1)$coef
  betanows[j,] <- betanow
  j <-j + 1
}
plot(betanows[,1],type="l",ylim = c(-50,50),main = "beta coefficents vs # iterations",
     xlab = "# iterations", ylab = "beta coefficient values")
for(i in 2:16){
  lines(betanows[,i],col=i)
}




```

We see that the plot shows that the gibbs sampler make the regression coefficients converge to relatively small magnitudes after initially diverging from zero. We take the burn-in period to be the first 2000 iterations and make marginal posterior distribution histograms of the beta coefficients generated after the burn-in. 

```{r,echo = FALSE}
par(mfrow = c(2,2))
for(i in 1:16){
  hist(betanows[2001:10000,i],col=i, border = "white",breaks = 50,
       main = c("Histogram for beta coefficient no.",i),
       xlab = "beta coefficient value", ylab = "number of sample draws")
}
```

It can be seen that most of the beta coefficients have reached a stationary posterior mode value and have normal marginal posterior distributions around their posterior modes. A quick check on trceplot of beta coefficient no. 13 reveals that it is still climbing slowly towards more posivive values by iteration 10000, hence the uniform-distribution looking histogram.

Let us look and compare the point estimates of $\beta$ from 2.a and 2.c:

```{r,echo=FALSE}
options(scipen = -10)
paste("Point estimate from 2.a:")
as.vector(drawnbetaposterior)
paste("Point estimate from 2.c:")
betanows[dim(betanows)[1],]
```

It is seen that the two point estimates are very similar.

```{r,echo= FALSE}
spamindicators <- matrix(0,nrow = length(y),ncol = 2)
for(i in 1:length(y)){
  if(pnorm(X[i,] %*% betanows[dim(betanows)[1],]) >= 0.5){
    spamindicators[i,2] <- 1    
  }
  if(pnorm(X[i,] %*% as.vector(drawnbetaposterior)) >= 0.5){
    spamindicators[i,1] <- 1    
  } 
}

paste("Classification rate for coefficients drawn from posterior mode normal dist.: ",
      signif(100 * sum(y == spamindicators[,1]) /length(y),3), "%")
paste("Classification rate for gibbs sampler coefficients: ",
      signif(100 * sum(y == spamindicators[,2]) /length(y),3), "%")
```

We also see that the classification performance of the gibbs sampler coefficients is as good as the one obtained from coefficients found from the posterior mode normal distribution.
